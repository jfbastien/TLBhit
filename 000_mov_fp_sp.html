<!-- DO NOT EDIT -- episode notes autogenerated by TLBHit's render.py -->
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
    <title>TLB hit üí•</title>
    <style>
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 100; src: local('Alegreya Sans Thin'), local('AlegreyaSans-Thin'), url('./fonts/AlegreyaSans-Thin.woff2') format('woff2'), url('./fonts/AlegreyaSans-Thin.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 300; src: local('Alegreya Sans Light'), local('AlegreyaSans-Light'), url('./fonts/AlegreyaSans-Light.woff2') format('woff2'), url('./fonts/AlegreyaSans-Light.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: italic; font-weight: 300; src: local('Alegreya Sans Light Italic'), local('AlegreyaSans-LightItalic'), url('./fonts/AlegreyaSans-LightItalic.woff2') format('woff2'), url('./fonts/AlegreyaSans-LightItalic.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 400; src: local('Alegreya Sans Regular'), local('AlegreyaSans-Regular'), url('./fonts/AlegreyaSans-Regular.woff2') format('woff2'), url('./fonts/AlegreyaSans-Regular.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 300; src: local('Alegreya Sans SC Light'), local('AlegreyaSansSC-Light'), url('../fonts/AlegreyaSansSC-Light.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-Light.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 400; src: local('Alegreya Sans SC Regular'), local('AlegreyaSansSC-Regular'), url('../fonts/AlegreyaSansSC-Regular.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-Regular.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 800; src: local('Alegreya Sans SC ExtraBold'), local('AlegreyaSansSC-ExtraBold'), url('../fonts/AlegreyaSansSC-ExtraBold.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-ExtraBold.woff') format('woff'); font-display: fallback; }

      body { font-family: "Alegreya Sans", Arial, Helvetica, sans-serif; font-weight: 300; }
      header { max-width: 50em; margin: 0 auto; padding: 0 1em; display: flex; flex-direction: row; flex-wrap: wrap; justify-content: space-between; }
      header h1 { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 800; margin-bottom: 0; display: flex; flex-direction: column; }
      header h1 a { text-decoration: none; }
      header div { display: flex; flex-direction: column; margin: 1em 0 0 0; }
      header p { margin: 0; text-align: right; }
      header p:first-child { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 300; }
      h2 { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 400; }
      main { max-width: 50em; margin: 0 auto; padding: 0 1em; }
      article { padding: 1em 0; }
      ul { list-style-type: circle; }
      .grid .entry { padding: 1em 0 1em 0; display: flex; flex-direction: row; flex-wrap: wrap; justify-content: space-between; }
      .grid h3 { margin: 0; font-weight: 400; }
      .grid .entry .where { display: flex; flex-direction: column; flex-basis: 8em; }
      .grid p, .grid ul { margin: 0; }
      .grid ul { padding: 0 0 0 1em; }
      .grid .where p { font-weight: 100; }
      .grid .entry .what { display: flex; flex-direction: column; flex-basis: 30em; }
      .grid .short { padding: 0.2em 0 0.2em 0; }
      .grid .short .where { display: flex; flex-direction: row; }
      .grid .short .where h3, .grid .short .where p { width: 5em; }

      @media (prefers-color-scheme: dark) {
          body { background-color: #121212; color: white; }
          a:link { color: #bb86fc; }
          a:visited { color: #3700b3; }
      }

      .podcast-audio { width: 100%; }
      .playback-rate-controls ul { margin: 0; padding: 0; list-style: none; text-align: center; }
      .playback-rate-controls li { display: inline; }
      .playback-rate-controls li:not(:last-of-type)::after { content: " | "; }
      .playback-rate-controls ul li a { text-shadow: none; }
    </style>
  </head>
  <body>
    <header>
      <h1><a href='index.html'>TLB hit üí•</a></h1>
      <div>
      <p>A podcast about systems &amp; compilers</p>
      <p> üê¶ <a href="https://twitter.com/TLBhit">@TLBhit</a></p>
      </div>
    </header>
    <main>
<h1>Episode 0: <code>mov fp, sp</code></h1>
<audio id="audioplayer" src="https://traffic.libsyn.com/secure/tlbhit/tlbhit0.mp3" controls="controls" class="podcast-audio" preload="auto"></audio><div class="playback-rate-controls"><ul><li><a href="#" onclick="setPlaybackSpeed(0.5)">0.5‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(1)">1‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(1.25)">1.25‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(1.5)">1.5‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(1.75)">1.75‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(2)">2‚®â</a></li></ul></div>
<h2>00:00:00 Intro</h2>
<ul>
<li>Website: <a href="https://tlbh.it/">tlbh.it</a></li>
<li>Twitter: <a href="https://twitter.com/tlbhit">@tlbhit</a></li>
<li><a href="https://podcasts.apple.com/us/podcast/mov-fp-sp/id1538369465?i=1000496866078">This episode on Apple podcast</a></li>
<li>The stack pretty much always TLB hits!</li>
</ul>
<h2>00:00:59 Disclaimer</h2>
<ul>
<li>We're lifelong learners, only know so much!</li>
<li>Will put errata up on <a href="https://tlbh.it/">the TLB Hit website</a></li>
<li><a href="https://en.wikipedia.org/wiki/Covert_channel">"Sidechannels"</a> via Twitter</li>
</ul>
<h2>00:01:42 What's the stack?</h2>
<ul>
<li>Episode is named <code>mov fp sp</code></li>
<li><code>mov fp sp</code> in the prologue of functions</li>
<li>Epilogue has "reverse" <code>mov sp fp</code></li>
<li>Instructions that manipulate <em>the stack</em>!</li>
<li>Compiler spills values that registers can't hold onto the stack</li>
<li>Functions do it a lot -- have their own [local] state, call functions that
have their own state</li>
<li>Because subroutines can recurse without bounds would need unbounded number of
registers</li>
<li>Often different kinds of registers: arithmetic value registers, floating
point registers</li>
<li>Registers contain fairly arbitrary "stuff": pointers to data, pointers to
code, return addresses, etc.</li>
<li>Stack is <em>usually</em> contiguous and allocated on a per-thread basis</li>
<li>Idea of "GPRs": general purpose registers, though some machines have
dedicated registers for floating point values as well, or SIMD for really
wide</li>
<li>Prologue moves stack pointer to base pointer, epilogue moves base pointer
back to stack pointer, "undoing", locally manipulating the stack pointer then
rolling things back to where they previously were</li>
</ul>
<h2>00:03:50 Mechanisms in the processor</h2>
<ul>
<li>Frame pointer/base pointer (bp/fp), stack pointer (sp)</li>
<li>Usual convention is that the frame pointer doesn't change during the course
of the function's execution</li>
<li>Generated code addresses "slots" relative (at offsets from) the frame
pointer; e.g. <code>+4</code>, <code>+8</code>, etc.</li>
<li>Stack is kind of like a linked list! Pointer of the stack that says "this is
where the frame pointer <em>used to be</em> before we came into this routine".</li>
</ul>
<h2>00:05:10 Comparison to an abstract stack machine</h2>
<ul>
<li>In CS class you may learn about machines where you push two operands onto a
stack then do an add operation that consumes the top two things on the stack</li>
<li>Compare to traditional processor we use today: expanding the stack as a
single operation that makes a bunch of slots at once</li>
<li>The slots don't need to be consumed in a strictly stack-order fashion</li>
<li>Distinction of "stack machine" vs scratchpad-area style frame areas
that happen in stack-like fashion for subroutine calls</li>
</ul>
<h2>00:06:05 Some instruction set considerations</h2>
<ul>
<li>Considerations on modern machines for frequency of these operations and how
they fit in our instruction cache; e.g. on x86 <code>push</code>/<code>pop</code> are single byte
opcodes</li>
<li>On ARM we may have a "push multiple values" instruction; little CISC-y but you
do so commonly it may make some sense</li>
<li>ARMv7 had instruction allowed to push 16 registers (all GPRs) and increment
stack pointer. Yay RISC!</li>
</ul>
<h2>00:07:09 Compiler optimizations and stackiness</h2>
<ul>
<li>By moving things onto the stack -- code is constantly working with the things
in its stack frame</li>
<li>Locality, but also avoiding memory allocation subroutines (100s or 1000s of
cycle depending)</li>
<li>In scratchpad area values are tracked precisely in dataflow sort of style</li>
<li>Bring them "in" to the compiler, values becomes more trackable</li>
<li>SSA values vs arbitrary memory references</li>
<li>When structs are brought onto the stack the individual fields inside can be
broken apart and the component fields can be tracked as individual values</li>
<li>Often called "scalar replacement of aggregates" (e.g. in LLVM)</li>
<li>When we home them on the stack we can do our common optimizations, CSE, DCE;
if on the heap, may be a lot harder to to do</li>
<li>In managed languages (e.g. JavaScript, Java) would do escape analysis to show
it doesn't escape via heap to an unknown subroutine -- once placed on the
stack you can eliminate whole objects and just track sub-fields inside of it</li>
<li>Allows you to just "explode" the object itself and think about its component
fields individually and get rid of whatever doesn't matter in there</li>
</ul>
<h2>00:09:17 Eliding heap allocations in C++</h2>
<ul>
<li>Some compilers can also sometimes optimize local heap allocations, turn them
into stacky allocation</li>
<li>C++ explicitly allows you to do that as of a few years ago, Clang does that</li>
<li>If you new an object no guarantee that you're actually going to put it on the
heap / call the underlying allocator</li>
<li>Can be surprising to people -- can do SRoA, other stuff, might get rid of the
entire computation</li>
<li>Neat, unless it's not what you're trying to do</li>
<li>But seems like a key optimization to do</li>
<li>If you're thinking about things as objects instead of raw bytes having higher
level understanding you can optimize based off of is pretty key it seems?</li>
</ul>
<h2>00:10:18 Frame pointer omission</h2>
<ul>
<li>When JF started programming there was "frame pointer omission" (FPO) which
was cool because optmizers weren't as good as they are now</li>
<li>Back when you only had 8 registers for x86 the extra register could go a long
way potentially -- stack is hot in cache but doing stores and loads to memory
locations</li>
<li>Was known to some as "that flag that makes the debugger way worse" -- debug
information has to be a lot more prescriptive when you can't simply describe
where things are as an offset from a canonical (assumed unchanging) register</li>
<li>Modern CPUs doing register renaming under the hood against a much bigger
micro-architectural register set -- not as worried about saving that one
register as much of the time -- although in hot code you still might</li>
</ul>
<h2>00:11:49 "Leaf" functions</h2>
<ul>
<li>When you inline things you make bigger regions for analysis, ideally make big
fat leaf functions</li>
<li>How much of program time is generally spent in leaf functions over some set
of applications?</li>
<li>Function at the end of the call tree</li>
<li>If your subroutine doesn't call any other subroutines that's a nice property,
because now you know that everything at the end of the stack belongs to you,
you're just doing your work and popping back up to whoever called you</li>
<li>Inlining really unlocks power of leaf -- inlining into non-leaf-functions can
<em>make</em> them become the leaf</li>
<li>So long as you don't over-inline and the working set doesn't become too big
-- the compiler can know everything it does and have a good amount of work to
do</li>
<li>Small region in which you can analyze <em>everything</em>, like tiny little whole
program analysis</li>
</ul>
<h2>00:13:10 Why do we have a stack again?</h2>
<ul>
<li>Why can't we inline everything?</li>
<li>Two main issues: 1) don't necessarily know call graph for the whole program
2) recursion</li>
<li>If you knew where all the calls went (virtual/indirect/etc in your
translation unit and other ones in your program), and without recursion, you
wouldn't need a stack, you know a perfect call graph</li>
<li>For some of these you could avoid having a stack -- virtual functions but
only a few actually implementations of it, could change to test-and-branch</li>
<li>If you have a fully analyzable virtual dispatch it effectively just becomes a
switch, can potentially inline what the targets are</li>
<li>Control flow analysis takes indirect branch that can go anywhere and
enumerate the real set of possibilities (devirtualization within a
translation unit)</li>
<li>Fully analyzeable call graph is an interesting computer history topic:
FORTRAN77 classically able to do this (programs were restricted enough you
could analyze it)</li>
<li>XLA ML/array programming machine learning compiler has the same property
where the whole call graph is analyzeable so you can create a slab that's the
giant frame for the whole program you're optimizing and all allocations are
known-fixed size</li>
<li>Whole program call graph analyzeability lives on in these niche use cases!</li>
<li>In stark contrast, sometimes we need multiple different kinds of stacks at
the same time!</li>
<li>The JS engine would sometimes recur from JS calls through the VM runtime to
other JS code, and that would need to potentially create a sub-stack (!) --
multi stack problems exist beyond even just needing to analyze/manage a
single stack</li>
<li>Programming in FORTRAN is cool, for scientific code often trying to
solve a specific physics problem don't <em>usually</em> need those tools like
recursion or virtual functions</li>
<li>When everything is "monomorphized" -- you have big arrays of
fixed-value-types you can know everything about the world and really optimize
everything based off of it -- fun mode to be in for scientific computing code</li>
</ul>
<h2>00:16:34 Considerations beyond recursion and indirect calls?</h2>
<ul>
<li>Some languages use the stack for fast thread switching? Things like full
stackful coroutines?</li>
<li>Stacks in Go for example are not contiguous: more like C++ deque: linked list
of lists instead of one contiguous stack -- clever x86 code sequence that
makes it fast to find previous and next frame</li>
<li>Allows Go stacks to be distinct allocations -- each page-wise is one frame
and the next function has another frame -- can put multiple functions in one
allocation</li>
<li>Used to have really bad perf if you were in a hot loop and happened to
straddle that boundary</li>
<li>Coroutines in some languages ended up having some "stackless" stuff like
this, where the closure is heap allocated instead</li>
<li>C++ coroutines try to do away with all the heap allocations, but depends on
optimization level whether it can do that or not</li>
<li>Kind of similar for Objective-C blocks -- until recently always heap
allocated, started being stack allocated in last few years where they could</li>
<li>Language doesn't say whether stuff lives on the heap or not</li>
<li>Because stack is less constrained can live in different places, e.g. in Go</li>
<li>In some cases you remove the allocation entirely</li>
</ul>
<h2>00:18:25 Scaling to millions of threads?</h2>
<ul>
<li>If you want to be able to scale your concurrency assumptions to millions of
threads, you don't want to have huge stacks</li>
<li>Each thread has a stack, and if you have millions of threads you don't want
to be allocating too much</li>
<li>And need to be able to switch between those threads quickly</li>
<li>So raises the question: how do you usually size those stacks in the
per-thread context you have?</li>
<li>If you're doing tiny little operations; e.g. if every operation in your
program was conceptually a thread, you wouldn't want to allocate 512KiB every
time you did a tiny atomic operation</li>
</ul>
<h2>00:19:10 Managed languages putting frames on the heap</h2>
<ul>
<li>On the term "stackless": one of the <a href="https://greenlet.readthedocs.io/en/latest/">Python
"greenlet"</a> ("lightweight thread"
terminology) attempts was called <a href="https://github.com/stackless-dev/stackless/wiki">Stackless
Python</a></li>
<li>In managed languages like Python the frames can be allocated on the heap;
e.g. in CPython the frames are allocated on the heap</li>
<li>But you still have a native stack for your native program that's doing the
managed VM execution! (e.g. the interpreter code written in C)</li>
<li>Something that JITs can do is unify the stacks; so that the "interpreted"
code uses the same stack space as the virtual machine -- single native thread
of stack space</li>
<li>[ed: on <em>deoptimization</em> the JIT code can be reified into a
managed stack frame on the heap through a process called "On Stack
Invalidation"]</li>
<li>Frame is really just some space, spaces get linked together in the abstract
concept of a stack, both of those things can also happen on the heap -- no
reason this one virtual memory region that we call the stack is the place where
that <em>has</em> to happen</li>
</ul>
<h2>00:20:20 Stack sizes in practice</h2>
<ul>
<li>GPUs have thousands and thousands of threads (organized in warps), you end up
wanting to not want huge stacks</li>
<li>But typically on CPU between 512KiB and 2MiB in common environments (MacOS,
Linux, Windows)</li>
<li>Allocated at thread startup</li>
<li>Most platforms have API to change the size -- some can only do at runtime,
some can only do at startup</li>
<li>Being able to change the stack size is this "uneven" thing across platforms</li>
<li>That's common "userspace": if you look at kernel threads or embedded systems
it may be way smaller, XNU: 16KiB</li>
<li>Blowing the stack can be easy in these cases when you're handling interrupts
-- can end up blowing stack pretty easily so have to be careful</li>
</ul>
<h2>00:21:27 What are the biggest stack programs?</h2>
<ul>
<li>The reverse of "tiny stacks" environments is interesting -- what are the
biggest stack programs that exist? Who does that?</li>
<li>In some environments that set stack limit quite low, when you compile in
debug mode, you'll be spilling a bunch of things, and then you'll get a
random segfault because the frame got bigger than the initial stack
allocation was assuming or you hopped into the guard pages</li>
</ul>
<h2>00:22:08 Allocating default start size</h2>
<ul>
<li>Conceptually, doesn't all have to be allocated at the start do you?</li>
<li>Could grow on the fly as you determine that you need more space</li>
<li>Interesting mechanisms to do that at OS level and for managed languages you
could even locate where your stack segment is</li>
<li>Just allocate a page and virtually map the other ones (no physical backing),
fault when they get accessed, then map them in as needed</li>
<li>In managed languages when you know where all the pointers are you can
relocate all the pointers that had anything to do with the stack locations</li>
</ul>
<h2>00:22:58 Stacks working nicely with LRU caches</h2>
<ul>
<li>Younger frames are recently touched</li>
<li>Spatial and temporal locality are two things that caches exploit</li>
<li>Spatial: when I touch a piece of data might touch something right next to it</li>
<li>Temporal: when I touch a piece of data might touch it again quickly (close in
number of cycles / time)</li>
<li>Last-In-First-Out (LIFO) nature of stack and the predictable access pattern
-- we go down in addresses and pop back up -- lines up with CPU features like
LRU caches and address predictors trying to prefetch things you're going to
be pulling in</li>
<li>Function's frame is usually in the cache in memory as "recently used"</li>
</ul>
<h2>00:23:54 Broader question: how does memory in a modern computer work at all?</h2>
<ul>
<li>Talking to stick of DRAM there are values going over the wires, those are the
physical addresses</li>
<li>What the computer program sees is the notion of virtual addresses</li>
<li>Virtual memory introduced originally so we could share the physical memory on
a system more easily in shared environments</li>
<li>Also offer facilities like protection, etc.</li>
<li>Virtual memory really means "I have some address" -- modern system
[userspace] has effectively 47 bit addresses -- that value is really an
"alias" -- it that turns into a real physical location</li>
<li>The alias is potentially unique to each program -- two different programs
hypothetically could have different 47-bit values that the mapping could turn
into the same physical location [ed: this property is the bane of virtually
tagged caches]</li>
<li>Each one of these memory access instructions like a load or store needs to
translate 47 bit virtual address into a corresponding physical address</li>
<li>You very likely have less than 47 bits of memory in your system (128PiB!) --
maybe you only have 4GiB (32b) of physical memory</li>
<li>Can pretend the physical space is larger and do tricks like page things out
to disk, take things that would be backed by physical memory and store them
on disk: magic possibilities of the virtual memory subsystem!</li>
</ul>
<h2>00:25:55 TLB Hit!</h2>
<ul>
<li>Each one of these accesses where I take a virtual address and turn it into a
physical address </li>
<li>Can be expensive because OS has to maintain/walk a big table of how that
translation happens</li>
<li>But there's a translation cache in the hardware called a TLB (Translation
Lookaside Buffer)</li>
<li>Because the stack is accessed so frequently and is small in terms of number
of pages, and pages are frequently used -- each stack access will usually hit
inside of this translation cache</li>
<li>TLB hit! As opposed to a miss</li>
<li>Caches are smort</li>
<li>Should do an episode that talks about virtual memory subsystems in more
detail!</li>
<li>Stack is LRU-cache amenable structure, and other benefits to it as well</li>
<li>Long winded way of describing how we ultimately arrive at TLB hits</li>
</ul>
<h2>00:27:00 Store-to-load forwarding of stack values</h2>
<ul>
<li>Other advantages to stack too -- registers are really fast, caches are fast,
and go to main memory tier at some point</li>
<li>Modern CPUs have store-to-load forwarding -- "cache" that forwards values
from recent stores to loads without having to ask the cache</li>
<li>Store to load forwarding effectively an "L0" cache -- stack can often hit in
there -- many stack accesses can hit in this, acting as an L0 cache</li>
<li>[ed: alignment is another important property for being able to hit in the
store to load forwarding pipeline!]</li>
</ul>
<h2>00:27:57 Multiprocessing teaser!</h2>
<ul>
<li>Let's say I'm a CPU in a multi-core die and I do a store, and I do
store-to-load forwarding, but some other processor stored to the same
location -- shouldn't I have seen the store that the other processor did?</li>
<li>"Stuck" in "L0 cache" -- hasn't been written to cache yet -- will it snoop?
Coherency is working at the cache level.</li>
<li>How does it work, how can you think about it. Memory models! Once you're in
multiprocessing world store-to-load forwarding can be even more interesting
than a common optimization.</li>
<li>What properties do you want from your system? Are these single thread
improvements "great until they're not?!"</li>
</ul>
<h2>00:29:00 Sparsity of stack slot usage</h2>
<ul>
<li>Each stage is contiguous</li>
<li>How a function uses the stack can be super sparse!</li>
<li>Compiler allocates a slot for each named value in your program</li>
<li>"Coloring" problem -- some parts of the function might have disjoint use of
slots</li>
<li>Similar to register allocation -- set of resources is stack memory</li>
<li>Figures out which ones are mutually exclusion like register allocation</li>
<li>In if/else blocks there's mutual exclusion -- even if you reuse slots in that
way you end up with parts of the stack that are unused</li>
<li>You could split paths? But probably a lot more overhead for the compiler to
track the paths / stack implications and know how to roll them back</li>
<li>Do compilers ever split to optimize stack behavior? </li>
<li>Compiler will do partial outlining to make cold code less intrusive, force
any required spilling on that cold path -- maybe that reduces required stack
space to some degree?</li>
<li>Probably they need to unify their understanding of the stack at any given
program point</li>
</ul>
<h2>00:31:50 Where does the stack go?</h2>
<ul>
<li>Do addresses grow up or grow down?</li>
<li>If A calls B is A's frame addresses bigger than B's frame addresses?</li>
<li>These days it's usually down: earlier in the call stack has higher address
for its frame than its callees</li>
<li>Architecture like ARM is both bi-endian and bi-directional in its stack
growth</li>
<li>Nothing to do with the abstract notion of push/pop.</li>
<li>Direction is interesting in general -- x86 has a direction register bit!</li>
<li>Changes the direction the addresses iterate for REP (repeat) instructions.</li>
<li>There are these various convention(s) we adhere to based on ABI.</li>
<li>Different implementations do different things when they implement the ABI.</li>
</ul>
<h2>00:33:07 Seemingly arbitrary choices in computer systems</h2>
<ul>
<li>Always fun to see these -- little vs big endian, address map with 0 at top or
bottom?</li>
<li>Endianness from Gulliver's Travels: Lilliputians crack and egg on one side or
the other (big or little side) that led to holy war.</li>
<li>We have a "war" over whether the low bytes of a multi byte word go at the low
memory or high memory [byte] address, we similarly fight these wars over
interesting small issues [ed: tabs vs spaces, anybody?].</li>
<li>If you only had the ability to access a word, you wouldn't know what the
endianness of a machine is! If you can't observe the byte storage. Would need
to fix alignment as well -- or only have word addresses. Word.</li>
</ul>
<h2>00:35:05 Unwinding</h2>
<ul>
<li>Let's say you want to grab a backtrace from your program because it crashed
or there's a bug or want to know where it is for diagnostic purposes, or in
C++ you throw an exception and need to unwind it, any exception guards to
trigger? etc</li>
<li>Even just programming normal C you might want to capture a backtrace to emit
a good message on segfault or something, send crash diagnostics back to home
base in the case of browsers or something</li>
<li>[ed: or sampling profiling!]</li>
<li>Conceptually just want to walk the linked-list-like structure described earlier</li>
<li>Frame pointer to find older frame, look next to it to find the return address</li>
<li>Linkage between frames and code that caused the frame to be on the stack (via
return address)</li>
<li>Lots of cool libraries people have developed to do this: libunwind,
libunwind-llvm, libunwind-gcc, crash reporting libraries that wrap up
unwinding facilities e.g. breakpad -- what code was executing when something
bad like a crash happened?</li>
<li>Ability to walk stack structure and ask questions about it is key!</li>
</ul>
<h2>00:36:40 Walking with frame pointer omission?</h2>
<ul>
<li>If you think about a basic unwinder, all you have to do is walk the stack
itself -- tiptoe through the linked list.</li>
<li>But what if you elided frame pointers.</li>
<li>Side data structure that tells you how to determine the frame size for each
function.</li>
<li>Compiler knows what it is, so can emit "side data" in the program.</li>
<li>Unwinder needs to look at current program counter -- for callers that gets
pushed onto the stack (for the call), callee pops it into the program counter
to go back to where it was</li>
<li>Program counter can be used to look up the frame size data</li>
<li>Read only information that the compiler generates keyed off of program counter</li>
<li>Less info to carry on the stack, and only need to look up those side data
structures if you're trying to do unwinding</li>
<li>Goes into binary section like DWARF info if you're using ELF binaries, or
PCOFF or what have you on Windows</li>
<li>If you're doing unwinding have to call RAII destructors or finally blocks in
languages like Java</li>
<li>In other cases may want to do setjmp/longjmp with landing pads -- Win32 did
that on x86 to implement exception handling</li>
<li>As you unwind the stack you somehow have to figure out where your landing
pads are because those have the code that do the cleanup</li>
<li>For exceptions you have to run code: destructors, catch blocks, etc</li>
<li>Unwind one, look up landing pad, do some work, look up the next landing pad</li>
<li>If you have an exception that happens during the "work", have to deal with
that</li>
<li>On Windows it mixes Structured Exception Handling (SEH) with C++</li>
<li>If you have faults or divide by zero floating point you can handle it with
SEH</li>
<li>In Windows C++ code you can catch unrelated exceptions from SEH which can be
surprising</li>
<li>So with SEH you can write a trap handler in a C program -- might use signals
for in other environments like POSIX</li>
</ul>
<h2>00:40:35 What are setjmp/longjmp?</h2>
<ul>
<li>Way to save the current state of registers and where you are on the stack and
such</li>
<li>setjmp buffer -- saves it, longjmp you give the buffer and it puts the state
back the way it was before</li>
<li>setjmp is a function that can return "twice" (a la <code>clone()</code>) -- first time
it saves the state, when you longjmp back it tells you that the state has
been restored -- a function that can return twice!</li>
<li>Local state inside of the CPU, sometimes called a microcontext? Registers in
it and such to restore to make the local processor state look the way it was</li>
<li>Heap can't go back, but local processor state can go back to where it was</li>
<li>Similar to what we do when we enter kernel space in a way, stash a bunch of
state away so the kernel can start doing its thing, then you can resume</li>
<li>Abstract notion of continuations we may have to talk about at some future point</li>
</ul>
<h2>00:41:53 Variable sized stack entities!</h2>
<ul>
<li>We've been talking about frames being of a set size, but there are these
functions in C and (kind of) C++, called <code>alloca</code>, and you also have variable
length arrays (VLAs)</li>
<li>N bytes is the size, and declare it goes on the stack</li>
<li>Interesting topics: how are they optimized, what's their behavior when it's
dynamic, and such!</li>
<li>If you look at standard C++ there's no variable length arrays, common
extension to support C VLAs in C++ but not officially a thing that exists</li>
<li>Neat to be able to change the size of the stack frame at runtime</li>
<li>Small stack buffer optimizations using alloca; e.g. "only up to 256 bytes of
stuff, otherwise I'll heap allocate"</li>
<li>alloca is kind of an extension to the language in a way</li>
<li>alloca/malloc and then corresponding nothing/free before it exits the scope</li>
<li>alloca(0) technically undefined behavior, as is VLAs of 0</li>
<li>Can try to optimized based off of that undefined behavior, but it breaks
things!</li>
<li>What about VLA of negative value? Can't un-grow the stack!</li>
<li>Alloca/VLAs don't fail!</li>
<li>Is it pronounced alloca or "alloh-ka"? We're not sure, but we do know JF is
French-Canadian. Alloc, eh?</li>
<li>Returns a pointer to the stack location, can it return null? Would have to be
slower if it nominally could fail.</li>
<li>Part of language or OpenGroup POSIX standard? C Programming Language? We're
not sure, maybe part of POSIX.</li>
<li>Within LLVM everything is represented as alloca and creates stack out of
consolidated allocas, and put things into registers that it can.</li>
</ul>
<h2>00:45:31 Stack buffer overruns</h2>
<ul>
<li>We put lots of things on the stack, we see some are variable sized</li>
<li>Sometimes we put fixed size array on the stack and <em>think</em> we're writing into
the right way</li>
<li>But O.G. buffer overrun exploit was to overrun an array that somebody put on
a stack, then attacker gets you to clobber the return address -- write an
address into there, and maybe jump to the data I had written if the stack
were executable. Nowadays the stack is never executable.</li>
<li>Pages in virtual memory have an "execute" bit on them that says whether you
can fetch instructions into the CPU out of that virtual memory location.</li>
<li>Fairly new? CPUs 20 years ago couldn't do that</li>
<li>Fascinating all the security/mitigations added recently in computer history.</li>
<li>Originally learned about the buffer overrun exploit in school, and nominally
not possible to happen anymore, especially because compilers had facilities
for inserting protector guards and things</li>
<li>Exploit shows how everything mixes together in the stack: instruction
pointers, data you want to home there.</li>
<li>In a memory unsafe environment an attacker can potentially take advantage of
the fact that these all mix together, things like virtual table pointers and
such may be in there.</li>
<li>Lots of extra technologies came in later not just from CPU side but from
compiler trying to protect stuff.</li>
<li>Stack protector: when you create a large frame (say 8KiB stack array) --
statically (when frame is constructed) or dynamically (as the code executes)
will touch all the pages that the code might access. If you went to go access
the end right away, might run afoul of the guard page (e.g. if it's a 4KiB
single guard page).</li>
</ul>
<h2>00:48:56 What are guard pages?</h2>
<ul>
<li>When the OS creates a thread it'll map a certain amount of stack</li>
<li>It could do that lazily or eagerly</li>
<li>Imagine it eagerly allocates 16KiB, can put a 4KiB "guard page" at the end,
marked as not readable or writable</li>
<li>Intent is as you naturally run out of stack you'll fault, more will be
provided with physical backing, and execution will be resumed</li>
<li>Have been exploits in the past where programs just "jump over" accessing the
guard page and the exploit uses those accesses to do malicious things -- past
that guard page is other memory, either another stack, or a heap, etc.</li>
<li>Stack is just another part of the memory space!</li>
<li>Struct offset can be any value -- can have 4GiB of data in an inline array!</li>
<li>In the conventional sense where stack grows down and heap grows up, at some
point they might meet! Could jump from one into the other... it's all just
locations in memory.</li>
<li>Or if you have multiple stacks maybe stack can collide with another stack.</li>
<li>At the end of the day we have this big flat address space in an abstract
sense on modern machines so jumping around inside you have to know the
extents.</li>
<li>Brings up topics like how segment registers have evolved on x86: future
episode!</li>
</ul>
<h2>00:51:20 The Red Zone</h2>
<ul>
<li>Speaking of x86 evolution! Sounds cool... <em>Red Zone</em>.</li>
<li>Idea you can guarantee some space underneath your frame; e.g. 128B on x86-64,</li>
<li>As leaf function, you know there's extra space that's guaranteed to be there,
avoid accounting and clobber stuff relative to frame pointer without having
to do accounting.</li>
<li>Part of ABI that that extra space exists there.</li>
</ul>
<h2>00:52:00 Alignment!</h2>
<ul>
<li>Usually you always try to have an aligned stack. Byte aligned stack is weird
when most of the point is to spill registers and registers are 4B or 8B.</li>
<li>When ARMv8 was being designed all stack specific instructions (manipulating,
aligning) had to fault if the stack itself wasn't aligned properly</li>
<li>Expected any stack manipulation spills at least <em>two</em> 64-bit registers --
invariant: aligned to 16B all the time!</li>
<li>Instructions not just specialized to the stack, also trying to find incorrect
usage.</li>
<li>More you know about how your system <em>should</em> be working, the easier it would
be to detect if something is trying to be exploited or going off the rails
potentially.</li>
<li>Invariants about how system should be operating get stronger.</li>
<li>Prevents people from doing</li>
<li>super-creative-if-somewhat-dangerous-in-the-general-case stuff like "futurist
programming" idea: self modifying code tight inner loops to get all the
efficiency of a modern machine.</li>
<li>Just to say: some things can become more difficult as we start make this
general process around how things are supposed to work.</li>
</ul>
<h2>00:53:45 Processes vs performance</h2>
<ul>
<li>Establishing processes and conventions for "how a machine is used" vs
performance</li>
<li>Classic considerations around what registers are saved by caller vs those
saved by the callee, can have a big impact on performance</li>
<li>On architectures that by convention had purposes on different machines could
make these considerations even more interesting; mul using ecx, rep using edi</li>
<li>Questions of "what do we save away when switching from userspace to
kernelspace" -- some set of resources that's not too small but also not
overly large</li>
<li>Kernel threads having full separate stack once they leave interrupts to go to
full kernel thread</li>
<li>Big space of conventions around performance and the stack that's always
fascinating</li>
</ul>
<h2>00:55:45 Managed Virtual Machines: WebAssembly</h2>
<ul>
<li>In WebAssembly all the code is totally separate from the data</li>
<li>WebAssembly meant to be secure VM that can run inside of the browser</li>
<li>Most C/C++ programs use von Neumann machine model where data and code are
mixed together</li>
<li>WebAssembly follows Harvard architecture (code and data are kept separate)</li>
<li>Writing C/C++/Rust program in WebAssembly you don't know where any of the
code is in an absolute sense</li>
<li>WebAssembly really has two stacks!</li>
<li>Call stack with internal state having function pointers etc.</li>
<li>Whatever state needs to be reified in your web assembly program; e.g. taking
address of variable, needs to see the values</li>
<li>Separate stack in the WebAssembly memory space separate from the call stack</li>
<li>If WebAssembly is running in a web browser it's interleaved with JS</li>
<li>Within WebAssembly memory there's a separate stack for your program's
execution -- part of the ABI</li>
<li>Extra neat: if you look at the virtual ISA it doesn't have a finite set of
registers!</li>
<li>Functions can have infinite registers, can return infinite set of values</li>
<li>"Virtual register" and "variadic return value arity"</li>
<li>Functions have fixed number of inputs</li>
<li>How does it do <code>printf</code>/<code>va_lists</code>?</li>
<li>Compiler creates a stack in user accessible memory (the WebAssembly stack)
you spill to</li>
<li><code>printf</code> routine can access those stack spills into WebAssembly memory</li>
</ul>
<h2>00:58:48 Whole stack'o browser stacks</h2>
<ul>
<li>In full browser, there's native stack, JavaScript stack which JIT can unify
with the native stack, also WebAssembly stack -- WebAssembly is also being
JIT compiled and potentially pushing things onto the native stack again!</li>
<li>Whole ecosystem of things all have interesting and potential unifying
treatment of the general notion of stacks</li>
<li>Like the glue that binds program execution together, in a way!</li>
<li>Super integral to how we think about how machines are going to work today</li>
</ul>
<h2>00:59:30 Managed VM's rich stack choices</h2>
<ul>
<li>In managed VMs you can potentially put more invariants on the program being
executed (you are managing what the language is doing)</li>
<li>CLR/.NET you have a typed stack [every slot has a known fixed type]</li>
<li>Projects like Pydgin mapped Python onto the typed stack, but CPython
has both data and control stack per frame, so figuring out how to mux Python
execution onto the CLR notion of stackiness in this managed world creates all
these interesting ideas/interactions</li>
<li>Around what a stack should be, how it should operate, what invariants we know
about it; e.g. whether the type of a given slot is always the same, or is it
a big bag of bytes</li>
<li>With typed arrays; are the typed arrays words or are they bytes, etc. Lots of
variations on how we might think about them.</li>
</ul>
<h2>01:00:40 What happens at the end of the stack?</h2>
<ul>
<li>In C++ when you stack overflow it's not defined where stack end is or what
happens when you run out of stack</li>
<li>Realistically what happens is you'll trap because of guard page in most
implementations</li>
<li>In JavaScript not defined <em>when</em> you stack overflow</li>
<li>Defined that <em>if</em> you stack overflow it a JavaScript exception is thrown, and
you can catch it!</li>
<li>Neat as a programmer, though it is kind of hard to handle</li>
<li>As a VM implementer it's kind of painful -- when you JIT code you're careful
about the JIT-ed code not running out of stack similar to stack protector</li>
<li>But runtime functions that are supposed to <em>support</em> the JIT use stack space,
and could violate the stack extent!</li>
<li>Now need to throw into the JavaScript code from a out-of-stack condition
inside native code</li>
<li>Can be a source of bugs in JS VMs -- JS fuzzers can do a cute "recurse, call
helper, unrecurse, call helper" to mess with VM to fuzz those runtime
functions interacting with stack overflow</li>
<li>Working on JS engine in FireFox you turn on the fuzzer when some feature is
ready and you get the deluge of interesting things that are corner cases in
the universe</li>
<li>Really good fuzzers out there for JavaScript: resulting code is evil, but
great stuff, that's what you want</li>
<li>Reducers are also the stars of the show -- getting the crisp short program
that shows some key problem is always amazing to see</li>
</ul>
<h2>01:03:48 Stack limit!</h2>
<ul>
<li>More things we haven't talked about: rotating register windows, return
predictors</li>
<li>[ed: ROP gadgets!]</li>
<li>Infinite stuff to talk about: unlike the stack!</li>
<li>Write to us on Twitter for feedback!</li>
</ul>
<h2>Terminology Glossary</h2>
<ul>
<li>VM / Virtual Machine: an overloaded term; however, most of the time that we
use it in the podcast we're referring to the notion that a managed language
implementation runs on an abstract machine that be thought of as designed for
that language in particular -- e.g. Python has a set of stack-based bytecodes
and associated runtime support that conceptually implements the abstract Python
virtual machine; similarly for the Java Virtual Machine and so on.</li>
<li>JIT: Just-in-Time compiler -- these are often used for speeding up managed
language environments by compiling managed programs to native code at runtime</li>
<li>"Stackless": "Stackless Python, or Stackless, is a Python programming
language interpreter, so named because it avoids depending on the C call
stack for its own stack." -- [<a href="https://en.wikipedia.org/wiki/Stackless_Python">https://en.wikipedia.org/wiki/Stackless_Python</a>]</li>
<li>reify: to "make real", i.e. materialize somehow in the computer or program;
antonym: erasure: erasing information from being present in the program that
had been determined in some prior phase like compilation</li>
<li><a href="https://en.wikipedia.org/wiki/Translation_unit_(programming">translation
unit</a>): the
input that is processed by a single compiler invocation (akin to a "module"
concept in languages with modules), serves as a natural scope for compiler
optimizations</li>
</ul>
    </main>
    <script>const setPlaybackSpeed = speed => { [].slice.call(document.querySelectorAll('audio,video')).map((e, idx) => { e.playbackRate = speed; }); }</script>
  </body>
</html>
