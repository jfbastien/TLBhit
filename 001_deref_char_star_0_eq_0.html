<!-- DO NOT EDIT -- episode notes autogenerated by TLBHit's render.py -->
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
    <title>TLB hit üí•</title>
    <style>
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 100; src: local('Alegreya Sans Thin'), local('AlegreyaSans-Thin'), url('./fonts/AlegreyaSans-Thin.woff2') format('woff2'), url('./fonts/AlegreyaSans-Thin.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 300; src: local('Alegreya Sans Light'), local('AlegreyaSans-Light'), url('./fonts/AlegreyaSans-Light.woff2') format('woff2'), url('./fonts/AlegreyaSans-Light.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: italic; font-weight: 300; src: local('Alegreya Sans Light Italic'), local('AlegreyaSans-LightItalic'), url('./fonts/AlegreyaSans-LightItalic.woff2') format('woff2'), url('./fonts/AlegreyaSans-LightItalic.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 400; src: local('Alegreya Sans Regular'), local('AlegreyaSans-Regular'), url('./fonts/AlegreyaSans-Regular.woff2') format('woff2'), url('./fonts/AlegreyaSans-Regular.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 300; src: local('Alegreya Sans SC Light'), local('AlegreyaSansSC-Light'), url('../fonts/AlegreyaSansSC-Light.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-Light.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 400; src: local('Alegreya Sans SC Regular'), local('AlegreyaSansSC-Regular'), url('../fonts/AlegreyaSansSC-Regular.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-Regular.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 800; src: local('Alegreya Sans SC ExtraBold'), local('AlegreyaSansSC-ExtraBold'), url('../fonts/AlegreyaSansSC-ExtraBold.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-ExtraBold.woff') format('woff'); font-display: fallback; }

      body { font-family: "Alegreya Sans", Arial, Helvetica, sans-serif; font-weight: 300; }
      header { max-width: 50em; margin: 0 auto; padding: 0 1em; display: flex; flex-direction: row; flex-wrap: wrap; justify-content: space-between; }
      header h1 { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 800; margin-bottom: 0; display: flex; flex-direction: column; }
      header h1 a { text-decoration: none; }
      header div { display: flex; flex-direction: column; margin: 1em 0 0 0; }
      header p { margin: 0; text-align: right; }
      header p:first-child { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 300; }
      h2 { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 400; }
      main { max-width: 50em; margin: 0 auto; padding: 0 1em; }
      article { padding: 1em 0; }
      ul { list-style-type: circle; }
      .grid .entry { padding: 1em 0 1em 0; display: flex; flex-direction: row; flex-wrap: wrap; justify-content: space-between; }
      .grid h3 { margin: 0; font-weight: 400; }
      .grid .entry .where { display: flex; flex-direction: column; flex-basis: 8em; }
      .grid p, .grid ul { margin: 0; }
      .grid ul { padding: 0 0 0 1em; }
      .grid .where p { font-weight: 100; }
      .grid .entry .what { display: flex; flex-direction: column; flex-basis: 30em; }
      .grid .short { padding: 0.2em 0 0.2em 0; }
      .grid .short .where { display: flex; flex-direction: row; }
      .grid .short .where h3, .grid .short .where p { width: 5em; }

      @media (prefers-color-scheme: dark) {
          body { background-color: #121212; color: white; }
          a:link { color: #bb86fc; }
          a:visited { color: #3700b3; }
      }

      .podcast-audio { width: 100%; }
      .playback-rate-controls ul { margin: 0; padding: 0; list-style: none; text-align: center; }
      .playback-rate-controls li { display: inline; }
      .playback-rate-controls li:not(:last-of-type)::after { content: " | "; }
      .playback-rate-controls ul li a { text-shadow: none; }
    </style>
  </head>
  <body>
    <header>
      <h1><a href='index.html'>TLB hit üí•</a></h1>
      <div>
      <p>A podcast about systems &amp; compilers</p>
      <p>üê¶ <a href="https://twitter.com/TLBhit">@TLBhit</a></p>
      <p>üéô <a href="https://tlbhit.libsyn.com/rss">RSS</a></p>
      <p>üçé <a href="https://podcasts.apple.com/us/podcast/tlb-hit/id1538369465">Apple podcast</a></p>
      </div>
    </header>
    <main>
<h1>Episode 1: <code>*(char*0) = 0</code>!</h1>
<h2>Intro [00:00]</h2>
<ul>
<li>Talked about TLBs last time, but didn't define what the acronym means last
podcast</li>
<li>Rectify that by talking about it way too much this time!</li>
<li>Episode title: <code>*(char*0) = 0</code>!</li>
<li>Standard disclaimer: only know so much, will try not to say things that are
wrong</li>
<li>Feel free to give us feedback!</li>
<li>Extensive show notes on the website (you're reading them right now!)</li>
</ul>
<h2>First Errata! [00:37]</h2>
<ul>
<li>Go no longer uses segmented stacks, does stack copying now</li>
<li>Mistakes is how you learn! [thanks to Graydon for pointing it out]</li>
</ul>
<h2>The Program [01:15]</h2>
<ul>
<li>How are we going to get to talk about TLB hits today?</li>
<li>Figure out how a program gets to a TLB hit, to help us defined what TLB is
and what hitting in it means</li>
<li>Propose analyzing a C program:<pre><code class="language-c">int main() {
  *(char *)0 = 0;
}
</code></pre>
</li>
<li>For people who don't have a teletypewriter inside of their head:<ul>
<li>main entry point</li>
<li>taking zero, turning it into a char star</li>
<li>deref-assigning it 0</li>
</ul>
</li>
<li>In a nutshell: storing zero to address 0 <code>(as a char*)</code></li>
<li>How can you not love this program?</li>
</ul>
<h2>What about its C++y cousin? [02:10]</h2>
<pre><code class="language-c++">int main() {
  *reinterpret_cast&lt;char*&gt;(nullptr) = 0;
}
</code></pre>
<ul>
<li>Use <code>nullptr</code> value and <code>reinterpret_cast</code> it</li>
<li>Can't <code>reinterpret_cast</code> nullptr, so won't compile -- #JustC++Things</li>
<li>As of C++20 can use <code>std::bit_cast</code>: just takes a value and moves the bits</li>
<li>Not guaranteed what you get when you bitcast/memcpy the nullptr!</li>
<li>Since <code>nullptr</code> is monostate (AKA existential) its contents are generally
irrelevant and thus not defined what its bit contents are</li>
<li>Old ways a bit more clear here</li>
</ul>
<h2>Why do we have both <code>bit_cast</code> and <code>reinterpret_cast</code>? [03:10]</h2>
<ul>
<li><code>bit_cast</code> takes the object representation's literal bits (as-if by memcpy)</li>
<li>Requires that the types (source and destination) have same size</li>
<li><code>bit_cast</code> returns a completely new object, whereas <code>reinterpret_cast</code>
returns a reference (or pointer)</li>
<li>Key here is that <code>std::nullptr_t</code> doesn't really have a value anyways</li>
<li>Tricky stuff around here like whether value types exist in the from-type or
to-type (but do any of us really exist?!)</li>
<li>Interesting to learn about the language lawyer-y aspects of C++ so we know
what we're typing in!</li>
</ul>
<h2>Getting back to the program! [04:55]</h2>
<ul>
<li><p>Dereferencing <code>NULL</code> and assigning it zero, and it's a <code>char*</code></p>
</li>
<li><p>What's cool talking about this program is everybody will look at it in a
different way from their perspective: language, compiler, OS/hypervisor,
instructions, hardware -- open, and a lot to talk about!</p>
</li>
<li><p>Dig into the basics!</p>
</li>
<li><p>Trying to reinterpret the value zero as a pointer, store another value, zero,
inside that memory location</p>
</li>
<li><p>Abstract translation from C to pseudo-assembly -- always a good exercise!</p>
<pre><code>r0 &lt;- 0
[r0] = st r0
</code></pre>
</li>
<li><p>Even without a particular CPU in mind, the translation from C to abstract
assembly is useful to know how to do</p>
</li>
</ul>
<h2>Does the compiler even let you compile this program? [06:45]</h2>
<ul>
<li>Writing something into null -- does the compiler &quot;let you&quot; turn it into
assembly or does it give you an error at compile time?</li>
<li>If it does give you an error at compile time, why?</li>
<li>Some people will run a static analyzer on their code to catch
definitely-null-deref errors like this, that can be determined statically</li>
<li>Probably won't come out of the box as part of normal C compiler</li>
<li>Stock gcc or clang will happily turn this into assembly instructions for me</li>
<li>Clang static analyzer will flag it -- tries to detect &quot;may-be-null
dereferences&quot; -- legitimate source of bugs in real code</li>
<li>People will often run static analyzers as part of presubmits or nightlies</li>
<li>&quot;If you take this path, you'll be dereferencing null&quot; analysis results</li>
<li>Other static analyzers like Coverity also do things like this</li>
<li>So at least the compiler will let us turn this into some binary (some set of
assembly instructions)</li>
<li>But now, we should note this is undefined behavior in both C and C++...</li>
</ul>
<h2>What does undefined behavior mean?</h2>
<ul>
<li>Generally means &quot;it can be exploited by the compiler that <code>$X</code> is not the
case&quot;</li>
<li><code>$X</code> in this case is &quot;I will not write to address 0 dynamically&quot; -- because
you're only allowed to write to actually existing objects</li>
<li>Compiler can use the fact it must be an existing object to do optimizations</li>
<li>If you actually do this at runtime then all bets are off for what the program
behavior actually is</li>
<li>Important thing: all a dynamic property -- if you have UB in part of your
code that never runs then it's not UB because it never executes! [For runtime
UB]</li>
<li>In this case if you actually execute that code it is UB</li>
<li>In C++ [and C] it likes to think about objects, everything is an object, and
if there is no object there you're not allowed to write to it, no lifetime
that has begun for that object</li>
<li>At a high level UB is not the compiler trying to be a jerk to the programmer</li>
<li>Allows compiler to assume that something isn't possible -- on the premise, if
it were to happen, it's a bug in your program</li>
<li>Ends up having kind of a ripple effect in code optimization -- each
individual optimization step makes sense but final result is often surprising
to people</li>
</ul>
<h2>Memory mapped register pattern [09:55]</h2>
<ul>
<li>Could make well defined by using <code>volatile</code> and using a non-zero value like
<code>1</code></li>
<li>Follows pattern of taking some number and casting to volatile <code>char*</code> or
<code>int*</code></li>
<li>See this often with hardware-defined (memory mapped, e.g. from a peripheral)
registers</li>
<li>Hardware vendor will say &quot;this address is a magical value, e.g. defined in
the spec, manual, or header file, and if you write to it [or read from it]
this is the behavior that you get out of the hardware&quot; -- it can do something
special!</li>
<li>So could use volatile address of <code>1</code>, and value could still be 0 we're
writing, it would still be well defined in that case.</li>
</ul>
<h2>Zero is usually not mapped in [10:35]</h2>
<ul>
<li>Zero is not mapped in to the address space is most modern userspace environments</li>
<li>In kernel mode or more niche architectures this could actually do something
-- could have peripheral with registers mapped in on address 0, or not fault
on writes to address 0 running some kind of embedded system -- on x86 in
physical memory the interrupt vector table resides at address 0</li>
</ul>
<h2>What does the assembly really look like? [11:15]</h2>
<ul>
<li><p>We talked about what the assembly would look like</p>
</li>
<li><p>Good to try it out in reality and see if you were wrong</p>
</li>
<li><p>Compiler explorer -- godbolt.org! gcc or llvm in different modes (x86, ARM)</p>
</li>
<li><p>Funny: without volatile, Clang on x86 just makes <code>ud2</code> (which just traps!)</p>
</li>
<li><p>Similarly makes <code>brk</code> on ARM64 which does similar</p>
</li>
<li><p>Code after optimization: enter main, then crash! (If you don't use volatile)</p>
</li>
<li><p>GCC a little &quot;nicer&quot; -- instead of just emitting <code>ud2</code> or <code>brk</code> it'll store
to <code>NULL</code> and <em>then</em> emit <code>ud2</code> or <code>brk</code> -- so it's &quot;doing what you asked it
to do&quot; and then crashes afterwards</p>
</li>
<li><p>But if you add volatile in there so you do:</p>
<pre><code class="language-c">int main() { *(volatile char*)0 = 0; }
</code></pre>
</li>
<li><p>On x86 you'll get a <code>mov</code>, on ARM a <code>strb</code> (byte store).</p>
</li>
<li><p>What's extra funny is GCC stil emits <code>ud2</code> after the null dereference even if
it's volatile. GCC even <em>ignores</em> the value being written! Changing to:</p>
<pre><code class="language-c">int main() { *(volatile char*)0 = 42; }
</code></pre>
<p>Still tries to store value 0!</p>
</li>
<li><p>A bit unintuitive and annoyed kernel folks in the past, but can tell GCC to
stop this with <code>-fno-delete-null-pointer-checks</code> -- unintuitive/frustrating</p>
</li>
</ul>
<p>to people around <code>volatile</code>, but!</p>
<ul>
<li><p>If we step back a bit, zero is a special value, but let's deref one instead,
gets rid of the silly <code>ud2</code></p>
<pre><code class="language-c">int main() { *(volatile char*)1 = 42; }
</code></pre>
<p>Now everything is like we expect.</p>
</li>
<li><p>Address zero really is treated specially by both clang and gcc</p>
</li>
<li><p>In this case volatile really tells the compiler when it sees address 1, that
there are some effects that it can't reason about might be happening --
&quot;something special, don't try to be too smart&quot;</p>
</li>
<li><p>This pattern if you have memory mapped hardware registers is legitimate to
access special memory and happens frequently!</p>
</li>
</ul>
<h2>Traps and faults and aborts, oh my! [14:30]</h2>
<ul>
<li>Back to focusing on user mode -- how does this instruction cause a trap, or
should we call it a fault?</li>
<li>Good question, maybe first can talk about exceptional things that happen in
the processor in general when running a program</li>
<li>Taxonomy of weird stuff that happens independent of normal flow of
instructions through the processor</li>
<li>Sometimes people break into categories of: traps, faults, and aborts</li>
<li>Trap: ask operating system for assist or to inspect what happened</li>
<li>Faults: something actually went wrong, and maybe can be corrected</li>
<li>Aborts: program basically has to terminate</li>
<li>For exceptions inside of the CPU, questions come up: do I need to show the
exact architectural state where this thing happened [what if I'm reordering
instructions under the hood?]</li>
<li>Differences between things like divide by zero [maybe I just need to know the
offending program counter?] and a system call, where your program is
&quot;cooperatively&quot; asking the operating system for something, and interrupts
that might come asynchronously to the program stream from a peripheral device
telling you it has a packet available or similar to be serviced</li>
<li>Relates questions like: can the hardware effectively reorder things assuming
that an address is non-null [or valid in general]: gets back to &quot;precise
exceptions&quot;</li>
<li>If a segfault happens, you want to be able to analyze the [exact] cause of
the segfault you got, instead of just &quot;skidding to a stop&quot; however it kept on
going in the program [for other instructions that were in flight]</li>
<li>Things inside of CPU like Reorder Buffers and the Point of No Return [and
retirement] -- at what point can you start clobbering your architectural
state with subsequent things that happen?</li>
<li>Popping back up to the compiler level there's a similar question: can the
<em>compiler</em> reorder things assuming a load is non-zero?</li>
<li>Even though the abstract machine in the language standard has no notion of
what a program looks like when something gets loaded from zero, compilers
bridge the &quot;practicality gap&quot; between abstract machine concept and what
people really expect their program to do when running against the hardware</li>
<li>Again goes back to the notion of implementation- or un-defined behavior</li>
</ul>
<h2>Even <em>more</em> bespoke circumstances! [16:45]</h2>
<ul>
<li>Traps or faults can happen <em>within</em> instructions</li>
<li>If you have repeated instructions like <code>rep movsb</code> (memcpy instruction),
canonical exmaple of composite CISC-y instruction</li>
<li>Or multi stack-push ARM instruction we talked about last episode</li>
<li>Also devices that send asynchronous interrupt the CPU potentially in the
middle of a long-running CISC-y instruction, how do we preempt that
instruction</li>
<li>Two traps racing: uniprocessor and an NMI comes in when exception is being
raised from somewhere else</li>
</ul>
<h2>Back to userspace virtual memory [17:45]</h2>
<ul>
<li><p>Memory accesses in user mode all dealing with virtual addresses, which we
talked a bit about last time</p>
</li>
<li></li>
<li><p>Two worlds of virtual and physical stuff, pretty complicated how one maps to
another</p>
</li>
<li><p>If you look at assembly, probably a third of instruction are memory accesses,
that's a lot! Lots of complication and a third of instructions have to deal
with it!</p>
</li>
<li><p>Guessing at reasoning for 1/3: conventional wisdom five instruction basic
block, one load and one store; therefore 2/5?</p>
</li>
<li><p>Technical term is a &quot;NPOOA&quot; -- order of magnitude, <em>lots</em> of instructions
needing to go through this process!</p>
</li>
<li><p>In computer systems always talking about &quot;fast path&quot; or &quot;common case&quot; because
if we can speed that up it might be large percentage of the workload, and
speeding up most important things [c.g. Amdahl's law]</p>
</li>
<li><p>In the common case when you've addressed the memory location recently you
want accesses to it to be fast, and caches are how this is made fast (we
talked a bit about this last time)</p>
</li>
<li><p>Some caches can be addressed virtually using the virtual addresses to lookup</p>
</li>
<li><p>Some can be address physically using the physical address you resolved from
the virtual address</p>
</li>
<li><p>Virtual address can be used for a &quot;virtually addressed cache&quot;, of course also
using the process context as an (effective) additional key, since it has its
own view of memory</p>
</li>
<li><p>As soon as you address physically (and most caches are address physically)</p>
</li>
<li><p>On x86 we have hardware page table walkers and so there's a control register
called CR3 that describes where the base of the page table structure lives</p>
</li>
<li><p>As a userspace programmer you're not used to the hardware having a contract
with your data structures, can be kind of mind boggling!</p>
</li>
<li><p>Granularity of the mapping we're talking about depends on memory pages, size
has to be dictated by hardware</p>
</li>
<li><p>Often the smallest page size will be something like 4KiB</p>
</li>
<li><p>Larger granularity called &quot;hugepages&quot; or &quot;superpages&quot;</p>
</li>
<li><p>Finding base of a page, just lop off the bottom bits to get the base of your
page</p>
</li>
</ul>
<h2>Zoom back out to the big picture [22:50]</h2>
<ul>
<li>Page table structure, lives in memory</li>
<li>Specialized piece of hardware that knows how all the structs look inside of
that page table structure</li>
<li>Contract between hardware and the operating system</li>
<li>Hardware capable of doing lookups to determine the virtual-to-physical
mapping</li>
<li>Slicing bits off the virtual address (from MSb to LSb) and walking
through the levels of the tree structure [each level has 1024 entries then it
slices 10 bits each time]</li>
<li>Potentially goes through four page table levels, four data dependent accesses</li>
<li>Ton of load instructions, four data dependent accesses would be quite
expensive</li>
<li>Sounds slow -- classic solution: add a cache!</li>
<li>Every memory access looks up in the TLB.</li>
<li>If the translation is found, TLB hit!</li>
<li>Otherwise, not found, TLB miss, makes us sad :-(</li>
<li>Every memory access, every instruction that does a memory access will look at
the TLB and see if it's a hit or miss, really interesting, making it so you
don't need to do the page table walk every memory access, makes a lot of
sense!</li>
<li>Important also that it only finds mapping for the current process/virtual
machine -- finding other process' translations would be a Bad Thing</li>
<li>TLB could be indexed using process context ID, but others are just flushed on
every context switch -- every time kernel touches CR3 flushes the TLB, can be
done on context switch</li>
<li>Also why it's more uncommon to see fully virtual caches, don't want two
process' views of memories confused, use a more physical notion instead of
needing to flush entries anytime a process switch occurs</li>
</ul>
<h2>How many entries? [25:50]</h2>
<ul>
<li>Can't be that many since it uses hardware implementing an associative array</li>
<li>Big parallel comparison on keys to figure out which value should be selected</li>
<li>Generally at most one should ever match</li>
<li>Gets into more detailed notion of how many cycles does it take to hit in the
L1 cache and what are those cycles of latency doing?</li>
<li>Usually ~3 cycles [correction: usually 4] on a super tight timing path --
look up entry in TLB and that tells us what the tag is that we compare
against for that address, and we look up the index in the cache (groups
addresses together), and then we resolve the index against the tag</li>
<li>Could be spelled out in more detail, but this couple-of-step process</li>
</ul>
<h2>Page Table Walkers As Little Accelerators [27:00]</h2>
<ul>
<li>TLB is software managed, when OS changes page table it flushes out the
entries from the TLB</li>
<li>Not totally sure, but think TLBs are pretty complete, aside from
invalidation, page table walkers can operate entirely autonomous from the OS
(don't require assistance)</li>
<li>Sometimes little fast accelerators can only handle common cases, but because
walking page table is a fairly straightforward process, can be pretty
complete and don't need to ask for help</li>
</ul>
<h2>Once physical translation is resolved [27:45]</h2>
<ul>
<li>On TLB hit then the hardware queries the cache and accesses the line if it's present; if not, cache protocol kicks in asking the next level</li>
<li>Should do an episode on how the caches line up and talk to each other</li>
<li>If not in any level will go fetch the line from DRAM</li>
<li>We should do a followup on DRAM row buffering and cache line replacement
policy and things like this involved in the memory subsystem</li>
</ul>
<h2>What about on TLB miss? [28:25]</h2>
<ul>
<li>If the TLB misses then a page table walk occurs -- either that'll find an
entry or it won't find the entry</li>
<li>So if the entry is found it'll be inserted into the TLB (want to find it next
time)</li>
</ul>
<h2>TODO</h2>
<ul>
<li>Programs install signal handlers for a variety of reasons</li>
<li>Can install to capture invalid memory accesses</li>
<li>Securing client code execution</li>
<li>&quot;Managed&quot; so prevent faulting</li>
<li>WebAssembly providing low level execution environment suitable for e.g. C++
or Rust</li>
</ul>
<h2>TODO</h2>
<ul>
<li>If indexing relative to a null object address, and you know the offset would
still be within the null page, can just catch the faulty access (via fault
handler) and resume by throwing a Java <code>NullPointerException</code> after that
handler had run [Advanced note: also requires on-stack-invalidation to happen
in the handler]</li>
</ul>
<h2>Rough &quot;Numbers to Know&quot; [42:30]</h2>
<ul>
<li>A lot of the numbers mostly similar to how they were when this talk was given</li>
<li>Talked about L1 cache hit could be order 3 cycles [edit: really 4 in modern
Intel machines]</li>
<li>Branch mispredicts: have to flush pipeline which is O(a dozen) pipe stages,
something like 16 cycles \approx 5ns.</li>
<li>L2 cache references, farther away, maybe 10 clocks; L3 cache references,
farther away still [and may have to cross slices], maybe 30 clocks</li>
<li>Main memory references can be O(100ns)</li>
<li>PCIe [RTT/2] traversal maybe 500ns away</li>
<li>These are various times &quot;local to the machine&quot; that could be interesting;
then other things like flash/spinning disk or communicating packets over the
network</li>
<li>Can imagine walking page table and doing bunch of main memory references,
four back to back main memory references potentially</li>
<li>Loads can vary from fraction of a nanosecond to hundreds of nanoseconds!</li>
<li>Really interesting because loads are a) at the heart of computing and b) have
such varied performance characteristics in the von Neumann machine model</li>
<li>Can go all the way to packets across the world and satellites and such!</li>
<li>Everybody likes good performance numbers!</li>
</ul>
<h2>Until Next Time [45:00]</h2>
<ul>
<li>A lot for now! Until next time...</li>
</ul>
    </main>
    <script>const setPlaybackSpeed = speed => { [].slice.call(document.querySelectorAll('audio,video')).map((e, idx) => { e.playbackRate = speed; }); }</script>
  </body>
</html>
