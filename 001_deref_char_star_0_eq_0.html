<!-- DO NOT EDIT -- episode notes autogenerated by TLBHit's render.py -->
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
    <title>TLB hit üí• Episode 1: `*(char*)0 = 0`!</title>
    <style>
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 100; src: local('Alegreya Sans Thin'), local('AlegreyaSans-Thin'), url('./fonts/AlegreyaSans-Thin.woff2') format('woff2'), url('./fonts/AlegreyaSans-Thin.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 300; src: local('Alegreya Sans Light'), local('AlegreyaSans-Light'), url('./fonts/AlegreyaSans-Light.woff2') format('woff2'), url('./fonts/AlegreyaSans-Light.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: italic; font-weight: 300; src: local('Alegreya Sans Light Italic'), local('AlegreyaSans-LightItalic'), url('./fonts/AlegreyaSans-LightItalic.woff2') format('woff2'), url('./fonts/AlegreyaSans-LightItalic.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans'; font-style: normal; font-weight: 400; src: local('Alegreya Sans Regular'), local('AlegreyaSans-Regular'), url('./fonts/AlegreyaSans-Regular.woff2') format('woff2'), url('./fonts/AlegreyaSans-Regular.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 300; src: local('Alegreya Sans SC Light'), local('AlegreyaSansSC-Light'), url('../fonts/AlegreyaSansSC-Light.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-Light.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 400; src: local('Alegreya Sans SC Regular'), local('AlegreyaSansSC-Regular'), url('../fonts/AlegreyaSansSC-Regular.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-Regular.woff') format('woff'); font-display: fallback; }
      @font-face { font-family: 'Alegreya Sans SC'; font-style: normal; font-weight: 800; src: local('Alegreya Sans SC ExtraBold'), local('AlegreyaSansSC-ExtraBold'), url('../fonts/AlegreyaSansSC-ExtraBold.woff2') format('woff2'), url('../fonts/AlegreyaSansSC-ExtraBold.woff') format('woff'); font-display: fallback; }

      body { font-family: "Alegreya Sans", Arial, Helvetica, sans-serif; font-weight: 300; }
      header { max-width: 50em; margin: 0 auto; padding: 0 1em; display: flex; flex-direction: row; flex-wrap: wrap; justify-content: space-between; }
      header h1 { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 800; margin-bottom: 0; display: flex; flex-direction: column; }
      header h1 a { text-decoration: none; }
      header div { display: flex; flex-direction: column; margin: 1em 0 0 0; }
      header p { margin: 0; text-align: right; }
      header p:first-child { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 300; }
      h2 { font-family: "Alegreya Sans SC", Arial, Helvetica, sans-serif; font-weight: 400; }
      main { max-width: 50em; margin: 0 auto; padding: 0 1em; }
      article { padding: 1em 0; }
      ul { list-style-type: circle; }
      .grid .entry { padding: 1em 0 1em 0; display: flex; flex-direction: row; flex-wrap: wrap; justify-content: space-between; }
      .grid h3 { margin: 0; font-weight: 400; }
      .grid .entry .where { display: flex; flex-direction: column; flex-basis: 8em; }
      .grid p, .grid ul { margin: 0; }
      .grid ul { padding: 0 0 0 1em; }
      .grid .where p { font-weight: 100; }
      .grid .entry .what { display: flex; flex-direction: column; flex-basis: 30em; }
      .grid .short { padding: 0.2em 0 0.2em 0; }
      .grid .short .where { display: flex; flex-direction: row; }
      .grid .short .where h3, .grid .short .where p { width: 5em; }

      @media (prefers-color-scheme: dark) {
          body { background-color: #121212; color: white; }
          a:link { color: #bb86fc; }
          a:visited { color: #3700b3; }
      }

      .podcast-audio { width: 100%; }
      .playback-rate-controls ul { margin: 0; padding: 0; list-style: none; text-align: center; }
      .playback-rate-controls li { display: inline; }
      .playback-rate-controls li:not(:last-of-type)::after { content: " | "; }
      .playback-rate-controls ul li a { text-shadow: none; }
    </style>
  </head>
  <body>
    <header>
      <h1><a href='index.html'>TLB hit üí•</a></h1>
      <div>
      <p>A podcast about systems &amp; compilers</p>
      <p>üê¶ <a href="https://twitter.com/TLBhit">@TLBhit</a></p>
      <p>üéô <a href="https://tlbhit.libsyn.com/rss">RSS</a></p>
      <p>üçé <a href="https://podcasts.apple.com/us/podcast/tlb-hit/id1538369465">Apple podcast</a></p>
      </div>
    </header>
    <main>
<h1>Episode 1: <code>*(char*0) = 0</code>!</h1>
<audio id="audioplayer" src="https://traffic.libsyn.com/secure/tlbhit/tlbhit1.mp3" controls="controls" class="podcast-audio" preload="auto"></audio><div class="playback-rate-controls"><ul><li><a href="#" onclick="setPlaybackSpeed(0.5)">0.5‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(1)">1‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(1.25)">1.25‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(1.5)">1.5‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(1.75)">1.75‚®â</a></li><li><a href="#" onclick="setPlaybackSpeed(2)">2‚®â</a></li></ul></div>
<h2>Intro [00:00]</h2>
<ul>
<li>Talked about TLBs last time, but didn't define what the acronym meant!</li>
<li>Rectify that by talking about them way too much this time</li>
<li>Episode title: <code>*(char*)0 = 0</code>!</li>
<li>Standard disclaimer: only know so much, will try not to say things that are
wrong, feel free to give us feedback</li>
<li>Extensive show notes on the website (you're reading them right now!)</li>
</ul>
<h2>First Errata! [00:37]</h2>
<ul>
<li>The Go Programming Language <a href="https://golang.org/doc/go1.3#stacks">no longer uses segmented
stacks</a>, does stack
copying now</li>
<li>Mistakes: how you learn! <a href="https://twitter.com/graydon_pub/status/1323503245717692417">Thanks to Graydon for pointing it
out!</a></li>
</ul>
<h2>The Program [01:15]</h2>
<ul>
<li>How are we going to talk about TLB hits today?</li>
<li>Figure out how a program gets to a TLB hit, to help us define what &quot;TLB&quot; is,
and what hitting in it means</li>
<li>Propose analyzing a C program:<pre><code class="language-c">int main() {
  *(char *)0 = 0;
}
</code></pre>
</li>
<li>For people who don't have a teletypewriter inside of their head:<ul>
<li>main entry point</li>
<li>taking zero, turning it into a char star</li>
<li>deref-assigning it 0</li>
</ul>
</li>
<li>In a nutshell: storing zero to address 0 (as a <code>char*</code>)</li>
<li>How can you not love this program?</li>
</ul>
<h2>What about its C++y cousin? [02:10]</h2>
<pre><code class="language-c++">int main() {
  *reinterpret_cast&lt;char*&gt;(nullptr) = 0;
}
</code></pre>
<ul>
<li>Use <code>nullptr</code> value and <code>reinterpret_cast</code> it</li>
<li>Can't <code>reinterpret_cast</code> nullptr, so won't compile -- #JustC++Things</li>
<li>As of C++20 can use <code>std::bit_cast</code>: just takes a value and moves the bits</li>
<li>Not guaranteed what you get when you bitcast/memcpy the nullptr!</li>
<li>Since <code>nullptr</code> is monostate [i.e. existential] its contents are generally
irrelevant and thus not defined what its bit contents are</li>
<li>Old ways a bit more clear here</li>
</ul>
<h2>Why do we have both <code>bit_cast</code> and <code>reinterpret_cast</code>? [03:10]</h2>
<ul>
<li><code>bit_cast</code> takes the object representation's literal bits (as-if by memcpy)</li>
<li>Requires that the types (source and destination) have same size</li>
<li><code>bit_cast</code> returns a completely new object, whereas <code>reinterpret_cast</code>
returns a reference (or pointer)</li>
<li>Key here is that <code>std::nullptr_t</code> doesn't really have a value anyways</li>
<li>Tricky stuff around here like whether value types exist in the from-type or
to-type (but do any of us really exist?!)</li>
<li>Interesting to learn about the language lawyer-y aspects of C++ so we know
what we're typing in!</li>
</ul>
<h2>Getting back to the program! [04:55]</h2>
<ul>
<li><p>Dereferencing <code>NULL</code> and assigning it zero, and it's a <code>char*</code></p>
</li>
<li><p>What's cool talking about this program is everybody will look at it in a
different way from their perspective: language, compiler, OS/hypervisor,
instructions, hardware -- open, and a lot to talk about!</p>
</li>
<li><p>Dig into the basics!</p>
</li>
<li><p>Trying to reinterpret the value zero as a pointer, store another value, zero,
inside that memory location</p>
</li>
<li><p>Abstract translation from C to pseudo-assembly -- always a good exercise!</p>
<pre><code>r0 &lt;- 0
[r0] = st r0
</code></pre>
</li>
<li><p>Even without a particular CPU in mind, the translation from C to abstract
assembly is useful to know how to do</p>
</li>
</ul>
<h2>Does the compiler even let you compile this program? [06:45]</h2>
<ul>
<li>Writing something into null -- does the compiler &quot;let you&quot; turn it into
assembly or does it give you an error at compile time?</li>
<li>If it does give you an error at compile time, why?</li>
<li>Some people will run a static analyzer on their code to catch
definitely-null-deref errors like this, that can be determined statically</li>
<li>Probably won't come out of the box as part of normal C compiler</li>
<li>Stock gcc or clang will happily turn this into assembly instructions for me</li>
<li>Clang static analyzer will flag it -- tries to detect &quot;may-be-null
dereferences&quot; -- legitimate source of bugs in real code</li>
<li>People will often run static analyzers as part of presubmits or nightlies</li>
<li>&quot;If you take this path, you'll be dereferencing null&quot; analysis results</li>
<li>Other static analyzers like Coverity also do things like this</li>
<li>So at least the compiler will let us turn this into some binary (some set of
assembly instructions)</li>
<li>But now, we should note this is undefined behavior in both C and C++...</li>
</ul>
<h2>What does undefined behavior mean?</h2>
<ul>
<li>Generally means &quot;it can be exploited by the compiler that <code>$X</code> is not the
case&quot;</li>
<li><code>$X</code> in this case is &quot;I will not write to address 0 dynamically&quot; -- because
you're only allowed to write to actually existing objects</li>
<li>Compiler can use the fact it must be an existing object to do optimizations</li>
<li>If you actually do this at runtime then all bets are off for what the program
behavior actually is</li>
<li>Important thing: all a dynamic property -- if you have UB in part of your
code that never runs then it's not UB because it never executes! [For runtime
UB]</li>
<li>In this case if you actually execute that code it is UB</li>
<li>In C++ [and C] it likes to think about objects, everything is an object, and
if there is no object there you're not allowed to write to it, no lifetime
that has begun for that object</li>
<li>At a high level UB is not the compiler trying to be a jerk to the programmer</li>
<li>Allows compiler to assume that something isn't possible -- on the premise, if
it were to happen, it's a bug in your program</li>
<li>Ends up having kind of a ripple effect in code optimization -- each
individual optimization step makes sense but final result is often surprising
to people</li>
</ul>
<h2>Memory mapped register pattern [09:55]</h2>
<ul>
<li>Could make well defined by using <code>volatile</code> and using a non-zero value like
<code>1</code></li>
<li>Follows pattern of taking some number and casting to volatile <code>char*</code> or
<code>int*</code></li>
<li>See this often with hardware-defined (memory mapped, e.g. from a peripheral)
registers</li>
<li>Hardware vendor will say &quot;this address is a magical value, e.g. defined in
the spec, manual, or header file, and if you write to it [or read from it]
this is the behavior that you get out of the hardware&quot; -- it can do something
special!</li>
<li>So could use volatile address of <code>1</code>, and value could still be 0 we're
writing, it would still be well defined in that case.</li>
</ul>
<h2>Zero is usually not mapped in [10:35]</h2>
<ul>
<li>Zero is not mapped in to the address space is most modern userspace environments</li>
<li>In kernel mode or more niche architectures this could actually do something
-- could have peripheral with registers mapped in on address 0, or not fault
on writes to address 0 running some kind of embedded system -- on x86 in
physical memory the interrupt vector table resides at address 0</li>
</ul>
<h2>What does the assembly really look like? [11:15]</h2>
<ul>
<li><p>We talked about what the assembly would look like</p>
</li>
<li><p>Good to try it out in reality and see if you were wrong</p>
</li>
<li><p>Compiler explorer -- godbolt.org! gcc or llvm in different modes (x86, ARM)</p>
</li>
<li><p>Funny: without volatile, <a href="https://godbolt.org/z/zxn88r">Clang on x86 just makes
<code>ud2</code></a> (which just traps! [but also gives a
nice warning])</p>
</li>
<li><p>Similarly makes <code>brk</code> on ARM64 which does similar</p>
</li>
<li><p>Code after optimization: enter main, then crash! (If you don't use volatile)</p>
</li>
<li><p>GCC a little &quot;nicer&quot; -- instead of just emitting <code>ud2</code> or <code>brk</code> it'll store
to <code>NULL</code> and <em>then</em> emit <code>ud2</code> or <code>brk</code> -- so it's &quot;doing what you asked it
to do&quot; and then crashes afterwards</p>
</li>
<li><p>But if you add volatile in there so you do:</p>
<pre><code class="language-c">int main() { *(volatile char*)0 = 0; }
</code></pre>
</li>
<li><p>On x86 you'll get a <code>mov</code>, on <a href="https://godbolt.org/z/Gc3erb">ARM a <code>strb</code></a>
(byte store).</p>
</li>
<li><p>What's extra funny is GCC stil emits <code>ud2</code> after the null dereference even if
it's volatile. <a href="https://godbolt.org/z/3PMWsE">GCC even <em>ignores</em> the value being
written!</a> Changing to:</p>
<pre><code class="language-c">int main() { *(volatile char*)0 = 42; }
</code></pre>
<p>Still tries to store value 0!</p>
</li>
<li><p>A bit unintuitive and annoyed kernel folks in the past, but can tell GCC to
stop this with <code>-fno-delete-null-pointer-checks</code> -- unintuitive/frustrating</p>
</li>
</ul>
<p>to people around <code>volatile</code>, but!</p>
<ul>
<li><p>If we step back a bit, zero is a special value, but let's deref one instead,
gets rid of the silly <code>ud2</code></p>
<pre><code class="language-c">int main() { *(volatile char*)1 = 42; }
</code></pre>
<p>Now everything is like we expect.</p>
</li>
<li><p>Address zero really is treated specially by both clang and gcc</p>
</li>
<li><p>In this case volatile really tells the compiler when it sees address 1, that
there are some effects that it can't reason about might be happening --
&quot;something special, don't try to be too smart&quot;</p>
</li>
<li><p>This pattern if you have memory mapped hardware registers is legitimate to
access special memory and happens frequently!</p>
</li>
</ul>
<h2>Traps and faults and aborts, oh my! [14:30]</h2>
<ul>
<li>Back to focusing on user mode -- how does this instruction cause a trap, or
should we call it a fault?</li>
<li>Good question, maybe first can talk about exceptional things that happen in
the processor in general when running a program</li>
<li>Taxonomy of weird stuff that happens independent of normal flow of
instructions through the processor</li>
<li>Sometimes people break into categories of: traps, faults, and aborts</li>
<li>Trap: ask operating system for assist or to inspect what happened</li>
<li>Faults: something actually went wrong, and maybe can be corrected</li>
<li>Aborts: program basically has to terminate</li>
<li>For exceptions inside of the CPU, questions come up: do I need to show the
exact architectural state where this thing happened [what if I'm reordering
instructions under the hood?]</li>
<li>Differences between things like divide by zero [maybe I just need to know the
offending program counter?] and a system call, where your program is
&quot;cooperatively&quot; asking the operating system for something, and interrupts
that might come asynchronously to the program stream from a peripheral device
telling you it has a packet available or similar to be serviced</li>
<li>Relates questions like: can the hardware effectively reorder things assuming
that an address is non-null [or valid in general]: gets back to &quot;precise
exceptions&quot;</li>
<li>If a segfault happens, you want to be able to analyze the [exact] cause of
the segfault you got, instead of just &quot;skidding to a stop&quot; however it kept on
going in the program [for other instructions that were in flight]</li>
<li>Things inside of CPU like Reorder Buffers and the Point of No Return [and
retirement] -- at what point can you start clobbering your architectural
state with subsequent things that happen?</li>
<li>Popping back up to the compiler level there's a similar question: can the
<em>compiler</em> reorder things assuming a load is non-zero?</li>
<li>Even though the abstract machine in the language standard has no notion of
what a program looks like when something gets loaded from zero, compilers
bridge the &quot;practicality gap&quot; between abstract machine concept and what
people really expect their program to do when running against the hardware</li>
<li>Again goes back to the notion of implementation- or un-defined behavior</li>
</ul>
<h2>Even <em>more</em> bespoke circumstances! [16:45]</h2>
<ul>
<li>Traps or faults can happen <em>within</em> instructions</li>
<li>If you have repeated instructions like <code>rep movsb</code> (memcpy instruction),
canonical exmaple of composite CISC-y instruction</li>
<li>Or multi stack-push ARM instruction we talked about last episode</li>
<li>Also devices that send asynchronous interrupt the CPU potentially in the
middle of a long-running CISC-y instruction, how do we preempt that
instruction</li>
<li>Two traps racing: uniprocessor and an NMI comes in when exception is being
raised from somewhere else</li>
</ul>
<h2>Back to userspace virtual memory [17:45]</h2>
<ul>
<li>Memory accesses in user mode all dealing with virtual addresses, which we
talked a bit about last time</li>
<li>Virtual addresses != physical addresses, the latter are morally linear &quot;DRAM
locations&quot; all lined up in physical memory</li>
<li>Each process in each guest VM has its own view of memory (virtual memory)
distinct from how others see it, all done through virtual memory address
layer</li>
<li>Two worlds of virtual and physical stuff, pretty complicated how one maps to
another</li>
</ul>
<h2>Lots of memory access instructions! [18:25]</h2>
<ul>
<li>If you look at assembly, probably a third of instruction are memory accesses,
that's a lot! Lots of complication and a third of instructions have to deal
with it!</li>
<li>Guessing at reasoning for 1/3: conventional wisdom five instruction basic
block, one load and one store; therefore 2/5?</li>
<li>Technical term is a &quot;NPOOA&quot; -- order of magnitude, <em>lots</em> of instructions
needing to go through this process!</li>
<li>In computer systems always talking about &quot;fast path&quot; or &quot;common case&quot; because
if we can speed that up it might be large percentage of the workload, and
speeding up most important things [c.g. Amdahl's law]</li>
<li>In the common case when you've addressed the memory location recently you
want accesses to it to be fast, and caches are how this is made fast (we
talked a bit about this last time)</li>
</ul>
<h2>Virtual vs physical addresses as cache keys [19:45]</h2>
<ul>
<li>Some caches can be addressed virtually using the virtual addresses to lookup</li>
<li>Some can be address physically using the physical address you resolved from
the virtual address</li>
<li>Virtual address can be used for a &quot;virtually addressed cache&quot;, of course also
using the process context as an [effective, if not materialized] additional
key, since it has its own view of memory</li>
<li>As soon as you address physically (and most caches are address physically),
need to somehow translate the virtual address to a physical one, and OS keeps
the mapping in a (hierarchical) table called the page table</li>
<li>Page table maps virtual address and process context/VM ID, mapping to
physical address and properties of memory location, like RWX permissions</li>
<li>At a high level organized as a tree, each level of tree is keyed off of bits
of the virtual address, because going to be pretty sparse, flattening would
make it huge, so needs a sparse (tree) layout</li>
<li>On x86 we have hardware page table walkers and there's a control register
called CR3 that describes where the base of the page table structure lives</li>
<li>What's cool is the kernel puts stuff in memory and tells the hardware that
location is special, and the hardware can start walking that autonomously</li>
<li>As a userspace programmer you're not used to the hardware having a contract
with your data structures, can be kind of mind boggling!</li>
<li>Granularity of the mapping we're talking about depends on memory pages, size
has to be dictated by hardware</li>
<li>Often the smallest page size will be something like 4KiB</li>
<li>Larger granularity called &quot;hugepages&quot; or &quot;superpages&quot;</li>
<li>Finding base of a page, just lop off the bottom bits to get the base of your
page</li>
</ul>
<h2>Zoom back out to the big picture [22:50]</h2>
<ul>
<li>Page table structure, lives in memory</li>
<li>Specialized piece of hardware that knows how all the structs look inside of
that page table structure</li>
<li>Contract between hardware and the operating system</li>
<li>Hardware capable of doing lookups to determine the virtual-to-physical
mapping</li>
<li>Slicing bits off the virtual address (from MSb to LSb) and walking
through the levels of the tree structure [each level has 1024 entries then it
slices 10 bits each time]</li>
<li>Potentially goes through four page table levels, four data dependent accesses</li>
<li>Ton of load instructions, four data dependent accesses would be quite
expensive</li>
<li>Sounds slow -- classic solution: add a cache!</li>
<li>Every memory access looks up in the TLB.</li>
<li>If the translation is found, TLB hit!</li>
<li>Otherwise, not found, TLB miss, makes us sad :-(</li>
<li>Every memory access, every instruction that does a memory access will look at
the TLB and see if it's a hit or miss, really interesting, making it so you
don't need to do the page table walk every memory access, makes a lot of
sense!</li>
<li>Important also that it only finds mapping for the current process/virtual
machine -- finding other process' translations would be a Bad Thing</li>
<li>TLB could be indexed using process context ID, but others are just flushed on
every context switch -- every time kernel touches CR3 flushes the TLB, can be
done on context switch</li>
<li>Also why it's more uncommon to see fully virtual caches, don't want two
process' views of memories confused, use a more physical notion instead of
needing to flush entries anytime a process switch occurs</li>
</ul>
<h2>How many entries? [25:50]</h2>
<ul>
<li>Can't be that many since it uses hardware implementing an associative array</li>
<li>Big parallel comparison on keys to figure out which value should be selected</li>
<li>Generally at most one should ever match</li>
<li>Gets into more detailed notion of how many cycles does it take to hit in the
L1 cache and what are those cycles of latency doing?</li>
<li>Usually ~3 cycles [correction: usually 4] on a super tight timing path --
look up entry in TLB and that tells us what the tag is that we compare
against for that address, and we look up the index in the cache (groups
addresses together), and then we resolve the index against the tag</li>
<li>Could be spelled out in more detail, but this couple-of-step process</li>
</ul>
<h2>Page Table Walkers As Little Accelerators [27:00]</h2>
<ul>
<li>TLB is software managed, when OS changes page table it flushes out the
entries from the TLB</li>
<li>Not totally sure, but think TLBs are pretty complete, aside from
invalidation, page table walkers can operate entirely autonomous from the OS
(don't require assistance)</li>
<li>Sometimes little fast accelerators can only handle common cases, but because
walking page table is a fairly straightforward process, can be pretty
complete and don't need to ask for help</li>
</ul>
<h2>Once physical translation is resolved [27:45]</h2>
<ul>
<li>On TLB hit then the hardware queries the cache and accesses the line if it's
present; if not, cache protocol kicks in asking the next level</li>
<li>Should do an episode on how the caches line up and talk to each other</li>
<li>If not in any level will go fetch the line from DRAM</li>
<li>We should do a followup on DRAM row buffering and cache line replacement
policy and things like this involved in the memory subsystem</li>
</ul>
<h2>What about on TLB miss? [28:25]</h2>
<ul>
<li>If the TLB misses then a page table walk occurs -- either that'll find an
entry or it won't find the entry (if there's no entry there)</li>
<li>So if the entry is found it'll be inserted into the TLB (want to find it next
time)</li>
<li>Some TLBs are managed by hardware and some by software -- who adds it to the
TLB depends on what the hardware platform dictates</li>
<li>Don't need to have hardware page table walkers -- could ask the OS when you
page fault &quot;can you walk this for me and put it into the TLB&quot;, then resume
the userspace program</li>
<li>Of course also have to check access permissions for the page; if you don't
have a &quot;W&quot; bit and you're doing a store, then a fault still needs to be
raised in that case</li>
</ul>
<h2>Code and data [29:30]</h2>
<ul>
<li>Complication of separate data and instruction caches</li>
<li>Code itself lives in memory!</li>
<li>Code is effectively data, even though it may be cached differently [when it's
being pulled into the instruction dispatch path]</li>
<li>Executing an arithmetic instruction like an <code>add</code> might cause a memory
access, because that actual instruction needs to be fetched!</li>
<li>Roughly a third of instructions cause memory access when they execute, but
<em>any</em> instruction could cause a memory access to be fetched</li>
<li>And that fetch is from virtual memory so can cause a page table walk, and
result in several memory accesses</li>
<li>In addition to instruction and data caches, modern systems have both
instruction and data TLBs</li>
<li>Different TLBs for instruction and data memory fetch paths, makes you wonder
what instruction can cause the most memory accesses on its own?!</li>
<li><a href="https://github.com/jbangert/trapcc">Folks on the internet showed MMUs are Turing
complete</a>, which ruins the fun
question -- if you stopped at &quot;simple&quot; instructions, then how could we
generate the most loads and stores from a single instruction</li>
<li>Say a scatter or gather instruction in the vector unit -- if that misses in
the iTLB (say 4-5 accesses) and then the gather itself touches 8 or 16
locations, and each could miss in the dTLB/cache</li>
<li>Maybe folks on the intertubes have ideas of what the maximum number of memory
accesses you could do in an instruction?!</li>
<li>The AVX gather instruction has an astounding amount of memory level
parallelism -- cool to see as new instructions are introduced what it does in
terms of questioning the assumptions of what we usually do with scalar code</li>
<li>Mind boggling to think about stores on the data path can be storing into
memory locations that are cached inside of the instruction cache [perhaps uop
translated or in-flight!] -- if need to be kept coherent then immediately the
store needs to be observable by the instruction side of the machine</li>
<li>On other [i.e. non x86] machines you need to explicitly flush the instruction
cache to perform a (non-coherent) writes into your instruction memory</li>
</ul>
<h2>IOMMUs: virtualized memory for devices [32:25]</h2>
<ul>
<li>Devices can also get a view of memory that's virtualized that can help
prevent e.g. evil PCIe devices, like if I made a malicious FPGA card that
wanted to grab things out of physical memory</li>
<li>Notion of IOMMUs, I/O Memory Management Units, can maintain a virtual memory
view for these peripheral devices</li>
<li>How you keep them consistent and such probably too much for this episode!</li>
</ul>
<h2>Back to initial example [33:00]</h2>
<ul>
<li>Virtual address was zero!</li>
<li>Some operating systems map zero page without any permissions so it'll always
cause a fault for every process</li>
<li>Global bit you can set on these entries to say it pertains to everybody</li>
<li>We don't want to map anything there to catch bugs</li>
<li>When you do mmap and you try to map something at 0 it doesn't have to handle
it specially because the entry is already there</li>
<li>But ends up acting similarly to if we load from an address that wasn't mapped
at all, then the page table walk would fail saying &quot;I don't see an entry for
this&quot; and causing a segmentation fault</li>
</ul>
<h2>So what is a segmentation fault? [33:45]</h2>
<ul>
<li>Hardware does the walk, and then tells the OS &quot;hey I couldn't find anything
(valid)&quot;</li>
<li>What the hardware ends up doing is using an ABI (Application Binary
Interface) between the hardware and the OS where the hardware then knows to
&quot;start executing $here&quot;</li>
<li>ABI defines what values will be in what registers, and from the kernel's
perspective, looks similar to a function call with a calling convention when
that interrupt handler is invoked</li>
<li>Can look like a regular (C) functional call when you look at it</li>
<li>Perhaps you get an enum that tells you what kind of trap/fault, say page
fault, and the address at which it occurred, and some notion of process
context identification</li>
<li>You'd find that in the architecture handbook/manual of how to program this
kind of machine -- what exactly happens when a CPU exception happens?</li>
<li>OS figures out which process it corresponds to and then decides what to do</li>
<li>If OS itself created an unexpected page fault maybe it panics!</li>
<li>If OS itself running in hypervisor, hyerpvisor might get first dibs on
handling hardware exception, pass it to guest OS</li>
<li>Guest OS might say &quot;that wasn't my problem&quot;, pass it on to the userspace
program in the form of a signal</li>
<li>Most userspace programs people don't really handle signals</li>
<li>Default signal handler gets triggered, and default handler might just say
&quot;thanks for playing!&quot; (e.g. &quot;segfault happened at $location&quot;) and then stop
the program; i.e. call abort</li>
</ul>
<h2>Running in debuggers [36:38]</h2>
<ul>
<li>If running program under a debugger will have installed signal handlers [for
its inferior] and report the issue</li>
<li>After reporting the issue to the developer will go back to the debugger
prompt</li>
<li>gdb and lldb are programs, running with a thin &quot;emulation&quot; layer [ed: maybe
more like &quot;supervisor&quot;?]</li>
<li>They talk to the kernel and they let the program pretend everything is
executing regularly when really they're &quot;inside&quot;/&quot;under&quot; the debugger</li>
<li>Other programs that do this kind of thing for a variety of reasons</li>
</ul>
<h2>JS engines / WebAssembly [37:35]</h2>
<ul>
<li>JS engines in browsers optimize things like WebAssembly by installing signal
handlers to capture invalid memory accesses</li>
<li>Dive into that, sounds a bit odd</li>
<li>Managed virtual machine running untrusted code from the internet, but
execution has to be secure</li>
<li>JS is a dynamic language that has semantics should prevent faulting from
happening</li>
<li>WebAssembly wants to provide a VM for something that looks like low level
static code (C++/Rust)</li>
<li>Associated with WebAssembly &quot;instance&quot; w/continuous 4GiB-limited slab of
memory -- currently 32-bit process model</li>
<li>On startup will map 4GiB of <em>virtual</em> memory with a redzone after, but it's
not physically backed -- allocating page table entries</li>
<li>Everything can be done as <code>base+offset</code> memory accesses using that virtual
base</li>
<li>Instead of checking &quot;have I mapped this&quot; every time, everything mapped
<code>-RWX</code> (from the start), unless user code has asked to map them</li>
<li>Hardware is telling you if it hasn't been mapped!</li>
<li>Ride on the existing fast paths -- way most VMs implement WebAssembly these
days is by having the hardware letting it know if something isn't mapped,
since hardware is good at that</li>
<li>Have to be sure access doesn't go past the 4GiB, 128MiB (say) of redzone</li>
<li>If the JIT can prove may go past that it will explicitly do an OOB check, but
anything inside it will just let trap</li>
<li>Can prove most memory accesses are within that bound in practice, rare to
have really large (known) offsets on loads and stores</li>
<li>If wasm access faults, browser signal handler checks whether it was a wasm
program -- signal handlers are process wide, so browser needs to figure out
if it's a wasm memory access, if so, unwind wasm stack (as in last episode)
and throw JavaScript exception (from the wasm access that faulted in
hardware)</li>
<li>If indexing relative to a null object address, and you know the offset would
still be within the null page, can just catch the faulty access (via fault
handler) and resume by throwing a Java <code>NullPointerException</code> after that
handler had run [Advanced note: also requires on-stack-invalidation to happen
in the handler]</li>
<li>Common technique for (Language) Virtual Machines in general!</li>
</ul>
<h2>Rough &quot;Numbers to Know&quot; [42:30]</h2>
<ul>
<li>We <em>did</em> cover what a TLB hit is, yay!</li>
<li>A lot of the numbers mostly similar to how they were when this talk was given</li>
<li>Talked about L1 cache hit could be order 3 cycles [edit: really 4 in modern
Intel machines]</li>
<li>Branch mispredicts: have to flush pipeline which is O(a dozen) pipe stages,
something like 16 cycles \approx 5ns.</li>
<li>L2 cache references, farther away, maybe 10 clocks; L3 cache references,
farther away still [and may have to cross slices], maybe 30 clocks</li>
<li>Main memory references can be O(100ns)</li>
<li>PCIe [RTT/2] traversal maybe 500ns away</li>
<li>These are various times &quot;local to the machine&quot; that could be interesting;
then other things like flash/spinning disk or communicating packets over the
network</li>
<li>Can imagine walking page table and doing bunch of main memory references,
four back to back main memory references potentially</li>
<li>Loads can vary from fraction of a nanosecond to hundreds of nanoseconds!</li>
<li>Really interesting because loads are a) at the heart of computing and b) have
such varied performance characteristics in the von Neumann machine model</li>
<li>Can go all the way to packets across the world and satellites and such!</li>
<li>Everybody likes good performance numbers!</li>
</ul>
<h2>Until Next Time [45:00]</h2>
<ul>
<li>A lot for now! Until next time...</li>
</ul>
<h2>Things we didn't get to!</h2>
<ul>
<li><code>perf stat</code> <a href="https://perf.wiki.kernel.org/index.php/Tutorial#Events">shows iTLB and dTLB
events</a>, but you do
always have to be careful looking up what the counters mean for your
particular architecture</li>
<li>One of the cool things about open cores like BOOM is you can just link to the
[implementation of a TLB implemented in</li>
</ul>
<p>Chisel](https://github.com/riscv-boom/riscv-boom/blob/1cb1596224c5681b839b8b115c1fbf6d802cc512/src/main/scala/lsu/tlb.scala)
  and <a href="https://github.com/chipsalliance/rocket-chip/blob/407496940311a0f0e8ec24627d93a7b839692ac6/src/main/scala/rocket/PTW.scala#L156">a page table
walker</a>!</p>
    </main>
    <script>const setPlaybackSpeed = speed => { [].slice.call(document.querySelectorAll('audio,video')).map((e, idx) => { e.playbackRate = speed; }); }</script>
  </body>
</html>
